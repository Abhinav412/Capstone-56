{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.wandb: WARNING Calling wandb.login() after wandb.init() has no effect.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\Scripts\\wandb.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\click\\core.py\", line 1157, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\click\\core.py\", line 1078, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\click\\core.py\", line 1688, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\click\\core.py\", line 1434, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\click\\core.py\", line 783, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\wandb\\cli\\cli.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\wandb\\cli\\cli.py\", line 253, in login\n",
      "    wandb.login(\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\wandb\\sdk\\wandb_login.py\", line 76, in login\n",
      "    return _login(\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\wandb\\sdk\\wandb_login.py\", line 311, in _login\n",
      "    wlogin.try_save_api_key(key)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\wandb\\sdk\\wandb_login.py\", line 176, in try_save_api_key\n",
      "    apikey.write_key(self._settings, key)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\wandb\\sdk\\lib\\apikey.py\", line 294, in write_key\n",
      "    raise ValueError(\n",
      "ValueError: API key must be 40 characters long, yours was 4\n",
      "wandb: Downloading large artifact fine-tuned-model:latest, 417.72MB. 1 files... \n",
      "wandb:   1 of 1 files downloaded.  \n",
      "Done. 0:0:4.5\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "wandb_api_key = os.getenv(\"wand_api_key\")\n",
    "\n",
    "!wandb login $wandb_api_key\n",
    "wandb.login()\n",
    "\n",
    "run = wandb.init(project='huggingface')\n",
    "artifact = run.use_artifact('fine-tuned-model:latest', type=\"model\")\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "model_path = f\"{artifact_dir}/model.pth\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.0553e-04, 3.0274e-03, 9.9637e-01]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = \"The stock market is performing exceptionally well today.\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'At multi-year highs: These 3 stocks witness 5-year swing high breakouts', 'source': 'Economictimes.com', 'published_at': '2025-03-25T03:12:52Z', 'url': 'https://m.economictimes.com/markets/stocks/news/at-multi-year-highs-these-3-stocks-witness-5-year-swing-high-breakouts/rising-high/slideshow/119453718.cms'}\n",
      "{'title': 'ATC Energies System IPO to open for subscription today. Check issue price, GMP, other details', 'source': 'The Times of India', 'published_at': '2025-03-25T02:14:41Z', 'url': 'https://economictimes.indiatimes.com/markets/ipos/fpos/atc-energies-system-ipo-to-open-for-subscription-today-check-issue-price-gmp-other-details/articleshow/119451851.cms'}\n",
      "{'title': 'Shri Ahimsa Naturals IPO to open for subscription today. Check issue price, GMP, other details', 'source': 'The Times of India', 'published_at': '2025-03-25T02:12:09Z', 'url': 'https://economictimes.indiatimes.com/markets/ipos/fpos/shri-ahimsa-naturals-ipo-to-open-for-subscription-today-check-issue-price-gmp-other-details/articleshow/119451776.cms'}\n",
      "{'title': 'Share Market Highlights: Sensex, Nifty hold steady; IT stocks shine, midcaps retreat', 'source': 'BusinessLine', 'published_at': '2025-03-25T01:01:06Z', 'url': 'https://www.thehindubusinessline.com/markets/stock-market-highlights-25-march-2025/article69368985.ece'}\n",
      "{'title': 'Bulls in beast mode, clean up D-Street losses', 'source': 'The Times of India', 'published_at': '2025-03-25T00:34:47Z', 'url': 'https://economictimes.indiatimes.com/markets/stocks/news/bulls-in-beast-mode-clean-up-d-street-losses/articleshow/119449425.cms'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "class NewsRetrievalAgent:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://newsapi.org/v2/everything?q=NSE\"\n",
    "\n",
    "    def fetch_news(self, query=\"IPO OR initial public offering\", days=7, sources=None):\n",
    "        date_from = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')\n",
    "        params = {\n",
    "            \"q\": query,\n",
    "            \"from\": date_from,\n",
    "            \"language\": \"en\",\n",
    "            \"sortBy\": \"publishedAt\",\n",
    "            \"apiKey\": self.api_key\n",
    "        }\n",
    "        if sources:\n",
    "            params[\"sources\"] = sources\n",
    "        \n",
    "        response = requests.get(self.base_url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return data.get(\"articles\", [])\n",
    "        else:\n",
    "            print(f\"Error fetching news: {response.status_code}\")\n",
    "            return []\n",
    "\n",
    "    def summarize_news(self, articles, top_n=5):\n",
    "        summaries = []\n",
    "        for article in articles[:top_n]:\n",
    "            summaries.append({\n",
    "                \"title\": article[\"title\"],\n",
    "                \"source\": article[\"source\"][\"name\"],\n",
    "                \"published_at\": article[\"publishedAt\"],\n",
    "                \"url\": article[\"url\"]\n",
    "            })\n",
    "        return summaries\n",
    "\n",
    "news_api_key = os.getenv(\"news_api_key\")\n",
    "news_agent = NewsRetrievalAgent(news_api_key)\n",
    "news_articles = news_agent.fetch_news()\n",
    "summaries = news_agent.summarize_news(news_articles)\n",
    "\n",
    "for news in summaries:\n",
    "    print(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At multi-year highs: These 3 stocks witness 5-year swing high breakouts {'negative': 0.0009772450430318713, 'neutral': 0.0020051468163728714, 'positive': 0.9970176219940186}\n",
      "ATC Energies System IPO to open for subscription today. Check issue price, GMP, other details {'negative': 0.001582072232849896, 'neutral': 0.9787472486495972, 'positive': 0.019670572131872177}\n",
      "Shri Ahimsa Naturals IPO to open for subscription today. Check issue price, GMP, other details {'negative': 0.0015661967918276787, 'neutral': 0.9793745279312134, 'positive': 0.01905926875770092}\n",
      "Share Market Highlights: Sensex, Nifty hold steady; IT stocks shine, midcaps retreat {'negative': 0.0014977715909481049, 'neutral': 0.12336458265781403, 'positive': 0.8751376271247864}\n",
      "Bulls in beast mode, clean up D-Street losses {'negative': 0.0012552207335829735, 'neutral': 0.9954403638839722, 'positive': 0.003304416546598077}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "model_path = f\"{artifact_dir}/model.pth\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "def analyze_sentiment(news_articles):\n",
    "    results = []\n",
    "    for article in news_articles:\n",
    "        text = article[\"title\"] + \" \" + article.get(\"description\", \"\")\n",
    "        inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        sentiment = torch.softmax(logits, dim=1).tolist()[0]\n",
    "        article[\"sentiment\"] = {\n",
    "            \"negative\": sentiment[0],\n",
    "            \"neutral\": sentiment[1],\n",
    "            \"positive\": sentiment[2]\n",
    "        }\n",
    "        results.append(article)\n",
    "\n",
    "    return results\n",
    "\n",
    "news_articles = news_agent.summarize_news(news_articles)\n",
    "news_with_sentiment = analyze_sentiment(news_articles)\n",
    "\n",
    "for news in news_with_sentiment:\n",
    "    print(news[\"title\"], news[\"sentiment\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 articles to nse_news_sentiment.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "model_path = f\"{artifact_dir}/model.pth\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "NEWS_URL = \"https://newsapi.org/v2/everything\"\n",
    "QUERY = 'NSE OR \"National Stock Exchange of India\"'\n",
    "LANGUAGE = \"en\"\n",
    "\n",
    "def fetch_nse_news():\n",
    "    params = {\n",
    "        \"q\": QUERY,\n",
    "        \"language\": LANGUAGE,\n",
    "        \"apiKey\": news_api_key\n",
    "    }\n",
    "    response = requests.get(NEWS_URL, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"articles\", [])\n",
    "    else:\n",
    "        print(f\"Error fetching news: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def analyze_sentiment(news_articles):\n",
    "    results = []\n",
    "    for article in news_articles:\n",
    "        text = article[\"title\"] + \" \" + article.get(\"description\", \"\")\n",
    "        inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        sentiment = torch.softmax(logits, dim=1).tolist()[0]\n",
    "        \n",
    "        article[\"sentiment\"] = {\n",
    "            \"negative\": sentiment[0],\n",
    "            \"neutral\": sentiment[1],\n",
    "            \"positive\": sentiment[2]\n",
    "        }\n",
    "        results.append(article)\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_to_csv(news_with_sentiment, filename=\"nse_news_sentiment.csv\"):\n",
    "    file_exists = os.path.exists(filename)\n",
    "    with open(filename, mode=\"a\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writerow([\"Date\", \"Title\", \"Source\", \"URL\", \"Positive\", \"Neutral\", \"Negative\"])\n",
    "\n",
    "        for article in news_with_sentiment:\n",
    "            writer.writerow([\n",
    "                datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "                article[\"title\"],\n",
    "                article[\"source\"][\"name\"],\n",
    "                article[\"url\"],\n",
    "                article[\"sentiment\"][\"positive\"],\n",
    "                article[\"sentiment\"][\"neutral\"],\n",
    "                article[\"sentiment\"][\"negative\"]\n",
    "            ])\n",
    "    print(f\"Saved {len(news_with_sentiment)} articles to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    news_articles = fetch_nse_news()\n",
    "    if news_articles:\n",
    "        news_with_sentiment = analyze_sentiment(news_articles)\n",
    "        save_to_csv(news_with_sentiment)\n",
    "    else:\n",
    "        print(\"No news articles retrieved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 articles to bse_news_sentiment.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "model_path = f\"{artifact_dir}/model.pth\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "NEWS_URL = \"https://newsapi.org/v2/everything\"\n",
    "QUERY = 'BSE OR \"Bombay Stock Exchange of India\"'\n",
    "LANGUAGE = \"en\"\n",
    "\n",
    "def fetch_nse_news():\n",
    "    params = {\n",
    "        \"q\": QUERY,\n",
    "        \"language\": LANGUAGE,\n",
    "        \"apiKey\": news_api_key\n",
    "    }\n",
    "    response = requests.get(NEWS_URL, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"articles\", [])\n",
    "    else:\n",
    "        print(f\"Error fetching news: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def analyze_sentiment(news_articles):\n",
    "    results = []\n",
    "    for article in news_articles:\n",
    "        text = article[\"title\"] + \" \" + article.get(\"description\", \"\")\n",
    "        inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        sentiment = torch.softmax(logits, dim=1).tolist()[0]\n",
    "        \n",
    "        article[\"sentiment\"] = {\n",
    "            \"negative\": sentiment[0],\n",
    "            \"neutral\": sentiment[1],\n",
    "            \"positive\": sentiment[2]\n",
    "        }\n",
    "        results.append(article)\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_to_csv(news_with_sentiment, filename=\"bse_news_sentiment.csv\"):\n",
    "    file_exists = os.path.exists(filename)\n",
    "    with open(filename, mode=\"a\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writerow([\"Date\", \"Title\", \"Source\", \"URL\", \"Positive\", \"Neutral\", \"Negative\"])\n",
    "\n",
    "        for article in news_with_sentiment:\n",
    "            writer.writerow([\n",
    "                datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "                article[\"title\"],\n",
    "                article[\"source\"][\"name\"],\n",
    "                article[\"url\"],\n",
    "                article[\"sentiment\"][\"positive\"],\n",
    "                article[\"sentiment\"][\"neutral\"],\n",
    "                article[\"sentiment\"][\"negative\"]\n",
    "            ])\n",
    "    print(f\"Saved {len(news_with_sentiment)} articles to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    news_articles = fetch_nse_news()\n",
    "    if news_articles:\n",
    "        news_with_sentiment = analyze_sentiment(news_articles)\n",
    "        save_to_csv(news_with_sentiment)\n",
    "    else:\n",
    "        print(\"No news articles retrieved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
